{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#For-the-4-different-pre­processing-specifications,-the-dimension-of-word-count-matrix\" data-toc-modified-id=\"For-the-4-different-pre­processing-specifications,-the-dimension-of-word-count-matrix-1\">For the 4 different pre­processing specifications, the dimension of word count matrix</a></span></li><li><span><a href=\"#for-the-4-different-pre-processing-specifications,-report-the-top-10-terms-with-the-“strongest”-coefficients\" data-toc-modified-id=\"for-the-4-different-pre-processing-specifications,-report-the-top-10-terms-with-the-“strongest”-coefficients-2\">for the 4 different pre-processing specifications, report the top 10 terms with the “strongest” coefficients</a></span></li><li><span><a href=\"#define-the-TF-and-IDF-you-want-to-use-for-this-dataset.\" data-toc-modified-id=\"define-the-TF-and-IDF-you-want-to-use-for-this-dataset.-3\">define the TF and IDF you want to use for this dataset.</a></span></li><li><span><a href=\"#for-the-4-different-pre-processing-specifications,-the-dimension-of-word-matrix\" data-toc-modified-id=\"for-the-4-different-pre-processing-specifications,-the-dimension-of-word-matrix-4\">for the 4 different pre-processing specifications, the dimension of word matrix</a></span></li><li><span><a href=\"#report-the-top-10-terms-with-the-“strongest”-coefficients-again\" data-toc-modified-id=\"report-the-top-10-terms-with-the-“strongest”-coefficients-again-5\">report the top 10 terms with the “strongest” coefficients again</a></span></li><li><span><a href=\"#comment-on-the-difference-between-with-and-without-applying-TF-IDF-when-predicting-the-job-title-using-these-different-features-in-a-paragraph.\" data-toc-modified-id=\"comment-on-the-difference-between-with-and-without-applying-TF-IDF-when-predicting-the-job-title-using-these-different-features-in-a-paragraph.-6\">comment on the difference between with and without applying TF-IDF when predicting the job title using these different features in a paragraph.</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer, ToktokTokenizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re # to remove hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed = json.load(open('indeed_title_job_desc.json', 'r'))\n",
    "job_key = list(indeed.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain the word counts for each job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "for key in job_key:\n",
    "    for job_des in indeed[key]:\n",
    "        title.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(Tokenizer, Stemmer):\n",
    "    data=[]\n",
    "    for key in job_key:\n",
    "        for job_des in indeed[key]:\n",
    "            # remove hyper link\n",
    "            job_des = re.sub(r'http\\S+', '', job_des, flags=re.MULTILINE) \n",
    "            # lower all the \n",
    "            # tokenize the sentense\n",
    "            toks = Tokenizer.tokenize(job_des)\n",
    "            # Stem the tokens \n",
    "            job_des = [Stemmer.stem(tok) for tok in toks]\n",
    "            # combine every loop result\n",
    "            data.append(job_des)\n",
    "    counts = [Counter(datum) for datum in data]\n",
    "    # frequenct matrix\n",
    "    df = pd.DataFrame(counts)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word count matrix for each pre-processing specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treebank_Porter = pre_process(TreebankWordTokenizer(), PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Treebank_Snow = pre_process(TreebankWordTokenizer(), EnglishStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokto_Porter =  pre_process(ToktokTokenizer(), PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Toktok_Snow = pre_process(ToktokTokenizer(), EnglishStemmer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the 4 different pre­processing specifications, the dimension of word count matrix\n",
    "- Using TreebankWordTokenizer, PorterStemmer generate 735 * 10981 word count matrix\n",
    "- Using TreebankWordTokenizer, SnowballStemmer (English Stemmer) generate 735 * 10874 word count matrix\n",
    "- Using ToktokTokenizer, PorterStemmer generate 735 * 11000 word count matrix\n",
    "- Using ToktokTokenizer, SnowballStemmer (English Stemmer) generate 735 * 10901 word count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10981)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Treebank_Porter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10874)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Treebank_Snow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 11000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokto_Porter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10901)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Toktok_Snow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top10(count_matrix):\n",
    "    X = count_matrix.fillna(0)\n",
    "    y = np.array(title)\n",
    "    logit = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    logit.fit(X, y)\n",
    "    coef_matrix = pd.DataFrame(data = logit.coef_).transpose()\n",
    "    coef_matrix.index = count_matrix.columns\n",
    "    coef_matrix.columns = job_key\n",
    "    coef_matrix = coef_matrix.assign(strength=lambda x:abs(x[job_key[0]])+ abs(x[job_key[1]])+\n",
    "                         abs(x[job_key[2]])+abs(x[job_key[3]])+abs(x[job_key[4]]))\n",
    "    coef_matrix = coef_matrix.sort_values(by=['strength'])\n",
    "    top10_result = list(coef_matrix.iloc[0:10,].index)   \n",
    "    return top10_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for the 4 different pre-processing specifications, report the top 10 terms with the “strongest” coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "Treebank_Porter_10 = top10(Treebank_Porter)\n",
    "Treebank_Snow_10 = top10(Treebank_Snow)\n",
    "Tokto_Porter_10 = top10(Tokto_Porter)\n",
    "Toktok_Snow_10 = top10(Toktok_Snow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for TreebankWordTokenizer, PorterStemmer:\n",
      "- usage.\n",
      "- dreams.\n",
      "- technology-rel\n",
      "- bachelors/mast\n",
      "- expansion.\n",
      "- potential.\n",
      "- consultingleadership\n",
      "- overnight\n",
      "- replac\n",
      "- EE\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for TreebankWordTokenizer, PorterStemmer:', *Treebank_Porter_10, sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for TreebankWordTokenizer, SnowballStemmer (English Stemmer):\n",
      "- expansion.\n",
      "- dreams.\n",
      "- usage.\n",
      "- bachelors/mast\n",
      "- potential.\n",
      "- technology-rel\n",
      "- building/manag\n",
      "- experienceproduct\n",
      "- confidence.\n",
      "- never-been-done-befor\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for TreebankWordTokenizer, SnowballStemmer (English Stemmer):', *Treebank_Snow_10, sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for  ToktokTokenizer, PorterStemmer:\n",
      "- expansion.\n",
      "- usage.\n",
      "- potential.\n",
      "- technology-rel\n",
      "- bachelors/mast\n",
      "- dreams.\n",
      "- consultingleadership\n",
      "- signature.\n",
      "- mi\n",
      "- building/manag\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for  ToktokTokenizer, PorterStemmer:', *Tokto_Porter_10, sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for ToktokTokenizer, SnowballStemmer (English Stemmer):\n",
      "- dreams.\n",
      "- technology-rel\n",
      "- usage.\n",
      "- potential.\n",
      "- expansion.\n",
      "- bachelors/mast\n",
      "- developer-readi\n",
      "- confidence.\n",
      "- never-been-done-befor\n",
      "- mis\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for ToktokTokenizer, SnowballStemmer (English Stemmer):', *Toktok_Snow_10, sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the TF and IDF you want to use for this dataset.\n",
    "\n",
    "- Term Frequenct (TF): the number of times a word $i$ occurs in a job description $d_{j}$ divide by the total counts of all words $k$ in a job description.\n",
    "\n",
    "- Inverse document frequency (IDF): the number N of job description divide by the number of job description $D$ containing a certain word, and take logarithm of the number .\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{TF}_{ij} = \\big(\\frac{n_{ij}}{\\sum n_{k,j}}\\big) + 1\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{IDF}_{i} = \\lg\\big(\\frac{D}{j: t_{i}\\in d_{j}}\\big)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{TF- IDF} = \\text{TF} \\times \\text{IDF}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_docs(Tokenizer, Stemmer):\n",
    "    docs=[]\n",
    "    for key in job_key:\n",
    "        for job_des in indeed[key]:\n",
    "            job_des = re.sub(r'http\\S+', '', job_des, flags=re.MULTILINE)  # remove hyper link\n",
    "            # tokenize the sentense\n",
    "            toks = Tokenizer.tokenize(job_des)\n",
    "            # Stem the tokens \n",
    "            job_des = [Stemmer.stem(tok) for tok in toks]\n",
    "            # combine every loop result\n",
    "            docs.append(job_des)\n",
    "    tfidf = TfidfVectorizer(analyzer='word',tokenizer=lambda x : x ,preprocessor=lambda x : x, token_pattern=None)\n",
    "    vect = tfidf.fit_transform(docs)\n",
    "    X = vect\n",
    "    y = np.array(title)\n",
    "    logit = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    logit.fit(X, y)\n",
    "    coef_matrix = pd.DataFrame(data = logit.coef_).transpose()\n",
    "    coef_matrix.index = tfidf.vocabulary_\n",
    "    coef_matrix.columns = job_key\n",
    "    coef_matrix = coef_matrix.assign(strength=lambda x:abs(x[job_key[0]])+ abs(x[job_key[1]])+\n",
    "                         abs(x[job_key[2]])+abs(x[job_key[3]])+abs(x[job_key[4]]))\n",
    "    coef_matrix = coef_matrix.sort_values(by=['strength'])\n",
    "    top10_result = list(coef_matrix.iloc[0:10,].index)   \n",
    "    return vect.shape, top10_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for the 4 different pre-processing specifications, the dimension of word matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using TreebankWordTokenizer, PorterStemmer generate 735 * 10981 word count matrix\n",
    "- Using TreebankWordTokenizer, SnowballStemmer (English Stemmer) generate 735 * 10874 word count matrix\n",
    "- Using ToktokTokenizer, PorterStemmer generate 735 * 11000 word count matrix\n",
    "- Using ToktokTokenizer, SnowballStemmer (English Stemmer) generate 735 * 10901 word count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "treepo=tfidf_docs(TreebankWordTokenizer(), PorterStemmer())\n",
    "treeen=tfidf_docs(TreebankWordTokenizer(), EnglishStemmer())\n",
    "topo=tfidf_docs(ToktokTokenizer(), PorterStemmer())\n",
    "toen=tfidf_docs(ToktokTokenizer(), EnglishStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10981)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treepo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10874)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 11000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735, 10901)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toen[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## report the top 10 terms with the “strongest” coefficients again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for tfidf:\n",
      "- requirements.\n",
      "- bessem\n",
      "- perfectlysuit\n",
      "- snowflake.net.\n",
      "- maintainable.\n",
      "- addepar.\n",
      "- /yearwork\n",
      "- easy-to-us\n",
      "- wholly-own\n",
      "- drinksfoosbal\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for tfidf:', *treepo[1], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for tfidf:\n",
      "- scale—it\n",
      "- domin\n",
      "- dashboards/visu\n",
      "- incred\n",
      "- theater\n",
      "- consumers.\n",
      "- idm\n",
      "- duties.\n",
      "- tcp/ip\n",
      "- outstand\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for tfidf:', *treeen[1], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for tfidf:\n",
      "- MO\n",
      "- efficiencies.\n",
      "- attempt\n",
      "- dip\n",
      "- trello\n",
      "- tools.key\n",
      "- python/scala/sql\n",
      "- metadata.\n",
      "- stibo\n",
      "- fundersclub\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for tfidf:', *topo[1], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 terms for tfidf:\n",
      "- data-fuel\n",
      "- environment.skil\n",
      "- headquarters.th\n",
      "- servant\n",
      "- hadoopazuregoogl\n",
      "- databusi\n",
      "- materials.\n",
      "- learningmarketingrpythonaffili\n",
      "- suck\n",
      "- rout\n"
     ]
    }
   ],
   "source": [
    "print('Top 10 terms for tfidf:', *toen[1], sep='\\n- ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comment on the difference between with and without applying TF-IDF when predicting the job title using these different features in a paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF reweight words to highlights those therm which are highly frequently in a certain document but less frequently across documents, to capture the pattern of certain types of documents. \n",
    "\n",
    "Without TF-IDF, general words which may apply to any job description get the strongest coefficients, like bachelors, dreams, usage, potential. While using TF-IDF, words specific to data science job would pop up in terms of coefficient strength, like data, dashboard, python/scala/sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
